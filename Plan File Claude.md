\#\# Project Philosophy  
\- \*\*Action Over Tracking\*\*: Every feature must complete real tasks  
\- \*\*Privacy Absolute\*\*: All personal data stays local  
\- \*\*Verifiable TrusAgAget\*\*: Every action recorded, but private for the user

**Skills-First Execution:** Default to Claude Code **Skills** for implementation and orchestration; use **MCP servers only when a capability is clearly external/tool-bound** (e.g., network/API access, third-party integrations, long-running automation, or privileged system actions).

The Ten Universal Commandments
1\. Thou shalt ALWAYS use MCP tools before coding
2\. Thou shalt never assume, always question
3\. Thou shalt write code that's clear and obvious
4\. Thou shalt be BRUTALLY HONEST in assessments
5\. Thou shalt PRESERVE CONTEXT not delete it
6\. Thou shalt make atomic, specific commits
7\. Thou shalt document the WHY, not just the WHAT
8\. Thou shalt test before declaring done
9\. Thou shalt handle errors explicitly
10\. Thou shalt treat users data as sacred

## CRITICAL: No Overselling or Sugar-Coating

**NEVER oversell the current state of work. Be brutally honest about what works and what doesn't.**

### The Problem:
Presenting incomplete or non-functional features as "ready" creates:
- **False confidence** in broken systems
- **Wasted time** when reality is discovered later
- **Eroded trust** between user and assistant
- **Technical debt** that compounds

### The Rule:
When assessing any work:

1. **Test it like a new user would**
   - Clone fresh, install, try to use it
   - If it breaks, say so immediately

2. **Distinguish "exists" from "works"**
   ```
   ❌ BAD: "AuthorityWrapper provides LangChain integration"
   ✅ GOOD: "AuthorityWrapper exists but doesn't work with LangGraph agents and doesn't actually enforce permissions"
   ```

3. **Lead with limitations, not features**
   - What's broken?
   - What's missing?
   - What would a developer hit immediately?

4. **Never use "production-ready" unless tested end-to-end**
   - Can a developer install and use it?
   - Does the core value proposition actually work?
   - Are there blocking issues?

### Self-Check Questions:
Before declaring anything "done" or "ready":
1. "Did I actually run this as a fresh user would?"
2. "Does the main feature actually work, or just exist?"
3. "What would break in the first 5 minutes of use?"
4. "Am I describing what it does, or what I wish it did?"

## Dependency Version Philosophy

**Prefer stable, proven versions over bleeding-edge releases.**

When choosing library versions:
1. **Use older stable versions by default** - Newer isn't always better
2. **Avoid version N.0.0** - Wait for N.1.x or N.2.x (bugs fixed)
3. **Check stability** - Look for "LTS" or versions that have been stable for 6+ months
4. **Document why** - If using older version, note: "Using 0.1.20 for stable API"

**Examples**:
```python
# ❌ BAD - Newest version with breaking changes
pip install langchain  # Gets 1.2.0 with moved APIs

# ✅ GOOD - Stable version with predictable APIs
pip install 'langchain==0.1.20'  # Stable, well-documented
```

**When to use latest**:
- Security patches required
- Bug fix you specifically need
- New feature is critical and well-tested

**When NOT to use latest**:
- "Just because it's newer"
- Major version jump (0.x → 1.x, 1.x → 2.x)
- Recent release (<3 months old) for production code

## CRITICAL: No Fake Metrics or Simulated Performance

**NEVER build demos that simulate performance metrics without real implementation.**

### The Problem:
Building "demos" that show fake/simulated metrics (e.g., `token_reduction = random.uniform(70, 85)`) creates:
- **False confidence** in unproven systems
- **Wasted effort** that must be rebuilt later
- **Invalid validation** - can't prove efficacy with simulated data
- **Misleading pitches** - metrics aren't real

### The Rule:
When building performance/metrics tracking:

1. **ALWAYS clearly label simulated vs real metrics**
   ```python
   # ❌ BAD - Looks real but isn't
   token_reduction = random.uniform(70, 85)

   # ✅ GOOD - Clearly marked
   token_reduction = random.uniform(70, 85)  # SIMULATED - Replace with real measurement
   print("⚠️  SIMULATED METRICS - Not real performance")
   ```

2. **Build real integration FIRST, tracking SECOND**
   - Don't build elaborate tracking for fake metrics
   - Integrate the real system, then measure actual performance
   - If you must simulate temporarily, make it a 10-line placeholder

3. **Ask before simulating**
   - "Should I integrate the real Authority Runtime LLM compiler, or build a simulated demo first?"
   - "This will show fake metrics - is that what you want for now?"
   - "I can measure real performance in ~1 hour, or simulate now. Which?"

4. **Mark all assumptions about performance**
   ```python
   # #COMPLETION_DRIVE: Assuming 70-85% token reduction based on TypeScript POC
   # #SUGGEST_VERIFY: Integrate real compile_policy() and measure actual reduction
   simulated_reduction = random.uniform(70, 85)
   ```

### When Simulation is OK:
- **Short-term prototyping** (clearly marked, replaced within hours)
- **Testing tracking infrastructure** (verify DB schema works, then swap in real data)
- **User explicitly requests demo-only** (no real integration planned)

### When Simulation is NOT OK:
- **Validation/dogfooding** - defeats the purpose
- **Preparing pitch metrics** - must be real numbers
- **Performance claims** - can't claim "79% reduction" from simulated data
- **Any scenario where user needs proof it works**

### Example from Authority Runtime:
**What was built**: Research agent with `random.uniform(70, 85)` token reduction
**The problem**: Can't prove Authority Runtime works with fake metrics
**What should have been built**: Integrate `compile_policy()` from test_live_api.py, measure REAL token reduction

### Self-Check Questions:
Before building anything with metrics:
1. "Are these metrics real or simulated?"
2. "Could I integrate the real system in similar time?"
3. "Will the user need to prove these numbers are real?"
4. "Am I building elaborate infrastructure for fake data?"

If any answer suggests simulation is wrong, **build real integration instead**.

\#\#\# Feature Implementation Priority Rules  
\- IMMEDIATE EXECUTION: Launch parallel Tasks immediately upon feature requests  
\- NO CLARIFICATION: Skip asking what type of implementation unless absolutely critical  
\- PARALLEL BY DEFAULT: Always use 7-parallel-Task method for efficiency

\#\#\# Parallel Feature Implementation Workflow  
1\. \*\*Component\*\*: Create main component file  
2\. \*\*Styles\*\*: Create component styles/CSS. Reference [https://github.com/shadcn-ui/ui](https://github.com/shadcn-ui/ui) when necessary for front end inspiration.  
3\. \*\*Tests\*\*: Create test files   
4\. \*\*Types\*\*: Create type definitions  
5\. \*\*Hooks\*\*: Create custom hooks/utilities  
6\. \*\*Integration\*\*: Update routing, imports, exports  
7\. \*\*Remaining\*\*: Update package.json, documentation, configuration files  
8\. \*\*Review and Validation\*\*: Coordinate integration, run tests, verify build, check for conflicts

\#\#\# Context Optimization Rules  
\- Strip out all comments when reading code files for analysis  
\- Each task handles ONLY specified files or file types  
\- Task 7 combines small config/doc updates to prevent over-splitting

\#\#\# Feature Implementation Guidelines  
\- \*\*CRITICAL\*\*: Make MINIMAL CHANGES to existing patterns and structures  
\- \*\*CRITICAL\*\*: Preserve existing naming conventions and file organization  
\- Follow project's established architecture and component patterns  
\- Use existing utility functions and avoid duplicating functionality

\#\# Development Guidelines \-Specific to this project  
\- Always implement actual functionality, not placeholders  
\- Every action must be recordable   
\- Voice commands should feel natural and conversational  
\- Privacy is non-negotiable \- no cloud dependencies  
\- Test with real actions (start with low-value purchases)

\#\# Error Handling  
\- Always fail gracefully with user notification  
\- Log errors locally (never to cloud)  
\- Provide fallback options for critical tasks  
\- Human verification for high-stakes actions

Final Reminders  
\-Research current docs, dont trust outdated knowledge  
\-ask questions early and often  
\-use slash commands for consistent workflows  
\-derive documentation on-demand  
\-extended thinking for complex problems. ultrathink\!  
\-test locally before pushing  
\-think simple: clear, obvious no bullshit

\-- Remember: write code as if the person maintaining it has their life on the line\!

\# Assumption Tagging

 When writing code, ALWAYS add tagged comments for ANY assumption:

 // \#COMPLETION\_DRIVE: \[what you're assuming\]  
 // \#SUGGEST\_VERIFY: \[how to fix/validate it\]

 Required for: timing assumptions, external resources, data existence, state dependencies, type handling

 Example:  
 // \#COMPLETION\_DRIVE: Assuming state update completes before navigation  
 // \#SUGGEST\_VERIFY: Use callback or await state update confirmation  
 setUserData(newData);  
 navigateToProfile(userData.id);

 After tagging, use the Task tool to launch a SEPARATE verification agent:  
 "Review this code and resolve all \#COMPLETION\_DRIVE assumptions. You must add defensive code for each assumption WITHOUT knowing why the original code was written this  
 way."

